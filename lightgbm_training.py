# -*- coding: utf-8 -*-
"""Darts-LightGBM-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VR_4cLKA8Zhc99Qt7cuRwTPflwMhXkzj
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import darts
import time
from tqdm import tqdm
import sklearn
from sklearn.preprocessing import OneHotEncoder
import pandas as pd
import torch
import gc

from darts import TimeSeries
from darts.models import LightGBMModel
from darts.dataprocessing import Pipeline
from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper
from darts.utils.timeseries_generation import datetime_attribute_timeseries
from darts.models import MovingAverage
from datetime import datetime, timedelta
from lightgbm import early_stopping
from darts.metrics import rmsle
# %matplotlib inline
torch.manual_seed(1); np.random.seed(1)  # for reproducibility

# Read CSV
def read_csv(path, file1 = 'train.csv', file2 = 'test.csv', file3 = 'holidays_events.csv',
             file4 = 'oil.csv', file5 = 'stores.csv', file6 = 'transactions.csv'):
    df_train = pd.read_csv(path + file1)
    df_test = pd.read_csv(path + file2)
    df_holidays_events = pd.read_csv(path + file3)
    df_oil = pd.read_csv(path + file4)
    df_stores = pd.read_csv(path + file5)
    df_transactions = pd.read_csv(path + file6)
    return df_train, df_test, df_holidays_events, df_oil, df_stores, df_transactions

# Create TimeSeries objects
def create_ts(group):
    list_of_ts = TimeSeries.from_group_dataframe(
                                    group,
                                    time_col="date",
                                    group_cols=["store_nbr","family"],
                                    static_cols=["city","state","type","cluster"],
                                    value_cols="sales",
                                    fill_missing_dates=True,
                                    freq='D')
    for ts in list_of_ts:
        ts = ts.astype(np.float32)
    list_of_ts = sorted(list_of_ts, key=lambda ts: int(ts.static_covariates_values()[0,0]))
    return list_of_ts

# Create TimeSeries objects for 33 product families
def ts_fam(ts_fam_dict, family_list, dataset):
    for family in tqdm(family_list):
        # Find where dataset['family'] = family
        df_family = dataset.loc[dataset['family'] == family]
        # Call create_ts() function
        # Extract individual time series by grouping 'df' by 'group_cols'
        # Meanwhile, extract additional columns as static covariates
        ts_fam_dict[family] = create_ts(df_family) 
    return ts_fam_dict

# Use pipeline (MissingValuesFiller->Scaler) to transform other data
def easy_data_transform(list_of_ts):
    # MissingValuesFiller
    filler = MissingValuesFiller(verbose=False, n_jobs=-1, name="Fill NAs")
    # Scaler
    scaler = Scaler(verbose=False, n_jobs=-1, name="Scaling")
    # Assemble a pipeline to process data
    pipeline = Pipeline([filler, scaler])
    # Fit and transform
    list_of_ts_transform = pipeline.fit_transform(list_of_ts)
    return list_of_ts_transform

# Use pipeline (MissingValuesFiller->StaticCovariatesTransformer->InvertibleMapper->Scaler) to transform sales data
def data_transform(list_of_ts):
    # MissingValuesFiller
    filler = MissingValuesFiller(verbose=False, n_jobs=-1, name="Fill NAs")
    # StaticCovariatesTransformer
    staticCov_transform = StaticCovariatesTransformer(verbose=False, transformer_cat = OneHotEncoder(), name="Encoder")
    # log transform
    log_transform = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name="Log-Transform")
    # Scaler
    scaler = Scaler(verbose=False, n_jobs=-1, name="Scaling")
    # Assemble a pipeline to process data
    pipeline = Pipeline([filler, staticCov_transform, log_transform, scaler])
    # Fit and transform
    list_of_ts_transform = pipeline.fit_transform(list_of_ts)
    return list_of_ts_transform, pipeline

# Transform sales data of 33 groups using data_transform()
def fam_sales_transform(ts_fam_dict, pl_fam_dict, ts_fam_transform_dict):
    # Call data_transform() function
    for key in tqdm(ts_fam_dict):
        ts_fam_transform_dict[key], pl_fam_dict[key] = data_transform(ts_fam_dict[key])
    return ts_fam_transform_dict, pl_fam_dict

# Compute the moving average of sales
def compute_sale_moving_avg(window, ts, col_names_new = 'moving_avg'):
    avg = window.filter(ts)
    avg = TimeSeries.from_series(avg.pd_series()).astype(np.float32)
    avg = avg.with_columns_renamed(col_names=avg.components, col_names_new = col_names_new)
    return avg

# Create 7-day and 28-day moving average of sales
def sales_moving_avg(sales_moving_avg_dict, transform_list, col_names_new_7 = "sales_ma_7", col_names_new_28 = "sales_ma_28"):
    sales_moving_avg_7 = MovingAverage(window=7)
    sales_moving_avg_28 = MovingAverage(window=28)
    for key in tqdm(transform_list):
        sales_mas_family = []
        for ts in transform_list[key]:
            sale_avg_7 = compute_sale_moving_avg(sales_moving_avg_7, ts, col_names_new = col_names_new_7)
            sale_avg_28 = compute_sale_moving_avg(sales_moving_avg_28, ts, col_names_new = col_names_new_28)
            mas = sale_avg_7.stack(sale_avg_28)
            sales_mas_family.append(mas)
        sales_moving_avg_dict[key] = sales_mas_family
    return sales_moving_avg_dict

# Create and transform time-based covariates
def time_based_transform_covar(sdate, edate):
    global_time = pd.date_range(start = sdate, end = edate, freq='D')
    year = datetime_attribute_timeseries(time_index = global_time, attribute="year")
    month = datetime_attribute_timeseries(time_index = global_time, attribute="month")
    day = datetime_attribute_timeseries(time_index = global_time, attribute="day")
    dayofyear = datetime_attribute_timeseries(time_index = global_time, attribute="dayofyear")
    weekday = datetime_attribute_timeseries(time_index = global_time, attribute="dayofweek")
    weekofyear = datetime_attribute_timeseries(time_index = global_time, attribute="weekofyear")
    timesteps = TimeSeries.from_times_and_values(times=global_time,
                                                 values=np.arange(len(global_time)),
                                                 columns=["linear_increase"])
    time_covar = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)
    time_covar = time_covar.astype(np.float32)
    time_covar_scaler = Scaler(verbose=False, n_jobs=-1, name="Scaler")
    time_covar_train, time_covar_val = time_covar.split_before(pd.Timestamp('20170816'))
    time_covar_scaler.fit(time_covar_train)
    time_covar_transform = time_covar_scaler.transform(time_covar)
    return time_covar_transform

# Compute 7-day and 28-day moving average 
def compute_moving_avg(transform_data, col_names_new_7, col_names_new_28):
    moving_avg_7 = MovingAverage(window=7)
    moving_avg_28 = MovingAverage(window=28)
    avg_7 = moving_avg_7.filter(transform_data).astype(np.float32)
    avg_7 = avg_7.with_columns_renamed(col_names=avg_7.components, col_names_new=col_names_new_7)
    avg_28 = moving_avg_28.filter(transform_data).astype(np.float32)
    avg_28 = avg_28.with_columns_renamed(col_names=avg_28.components, col_names_new=col_names_new_28)
    return avg_7, avg_28

# Create and transform 7-day and 28-day moving average of oil price
def oil_transform_moving_avg(df_oil):
    oil = TimeSeries.from_dataframe(df_oil, 
                                time_col = 'date', 
                                value_cols = ['dcoilwtico'],
                                freq = 'D').astype(np.float32)
    # Transform
    oil_transform = easy_data_transform(oil)
    
    # Moving Averages for Oil Price
    oil_moving_avg = []
    oil_avg_7, oil_avg_28 = compute_moving_avg(oil_transform, "oil_ma_7", "oil_ma_28")
    oil_moving_avg = oil_avg_7.stack(oil_avg_28)
    return oil_transform, oil_moving_avg

# Create 7-day and 28-day moving average of transactions for 54 stores
def transactions_transform_moving_avg(df_transactions):
    df_transactions.sort_values(["store_nbr","date"], inplace=True)
    # Extract individual transaction time series for 54 stores
    transactions_ts_list = TimeSeries.from_group_dataframe(
                                df_transactions,
                                time_col="date",
                                group_cols=["store_nbr"],
                                value_cols="transactions",
                                fill_missing_dates=True,
                                freq='D')
    
    # Remove static covariates
    transactions_ts_noCovar_list = []
    for ts in transactions_ts_list:
        transactions_ts_noCovar_list.append(TimeSeries.from_series(ts.pd_series()).astype(np.float32))
        
    # Check it later
    transactions_ts_noCovar_list[24] = transactions_ts_noCovar_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))
    
    # Since many stores open later than '20130101', we have to rescale all 54 stores time series
    # starting from '20130101' and ending at '20170815'
    rescale_transactions_ts_list = []
    for ts in transactions_ts_noCovar_list:
        if ts.start_time() > pd.Timestamp('20130101'):
            zero_ts_end_time = (ts.start_time() - timedelta(days=1))
            ts_gap = zero_ts_end_time - pd.Timestamp('20130101')
            zero_ts = TimeSeries.from_times_and_values(
                                      times=pd.date_range(start=pd.Timestamp('20130101'), 
                                      end=zero_ts_end_time, freq="D"),
                                      values=np.zeros(ts_gap.days+1))
            ts = zero_ts.append(ts)
            rescale_transactions_ts_list.append(ts)
        
    #Transform
    transactions_transform = easy_data_transform(rescale_transactions_ts_list)
    
    # transaction Moving Averages for 54 stores
    transactions_covar = []
    for ts in tqdm(transactions_transform):
        transactions_avg_7, transactions_avg_28 = compute_moving_avg(ts, "transactions_ma_7", "transactions_ma_28")
        transactions_moving_avg = ts.with_columns_renamed(col_names=ts.components, col_names_new="transactions").stack(transactions_avg_7).stack(transactions_avg_28)
        transactions_covar.append(transactions_moving_avg)
    return transactions_transform, transactions_covar

# Rephrase holiday events information
# holidays_events['transferred'] -> 'Transferred'
# holidays_events['Transfer'] -> 'Holiday'
# holidays_events['Additional'] -> 'Holiday'
# holidays_events['Bridge'] -> 'Holiday'
def rephrase_holiday(df_holidays_events):
    df_holidays_events['type'] = np.where(df_holidays_events['transferred'] == True,'Transferred', 
                                      df_holidays_events['type'])
    df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Transfer','Holiday', 
                                      df_holidays_events['type'])
    df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Additional','Holiday', 
                                      df_holidays_events['type'])
    df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Bridge','Holiday', 
                                      df_holidays_events['type'])
    return df_holidays_events

def holiday_list(df_stores, df_holidays_events):
    holidays_per_store = []
    for store in range(len(df_stores)):
        # Create holiday dataframe
        holiday = pd.DataFrame(columns=['date'])
        # Add holiday dates to holiday['date']
        holiday["date"] = df_holidays_events["date"]
        # If type = 'Holiday' and 'locale' = 'National' -> this date is "national_holiday" otherwise 0 
        holiday["national_holiday"] = np.where(((df_holidays_events["type"] == "Holiday") & (df_holidays_events["locale"] == "National")), 1, 0)
        # Add earthquake struck features
        holiday["earthquake_relief"] = np.where(df_holidays_events['description'].str.contains('Terremoto Manabi'), 1, 0)
        # Add christmas features
        holiday["christmas"] = np.where(df_holidays_events['description'].str.contains('Navidad'), 1, 0)
        # Add football_event features
        holiday["football_event"] = np.where(df_holidays_events['description'].str.contains('futbol'), 1, 0)
        # Add national_event features
        # Exclude earthquake struck and football_event
        holiday["national_event"] = np.where(((df_holidays_events["type"] == "Event") 
                                              & (df_holidays_events["locale"] == "National") 
                                              & (~df_holidays_events['description'].str.contains('Terremoto Manabi')) 
                                              & (~df_holidays_events['description'].str.contains('futbol'))), 1, 0)
        # Add work_day features
        holiday["work_day"] = np.where((pd.to_datetime(df_holidays_events["date"]).dt.dayofweek<5), 1, 0)
        holiday["work_day"]=np.where(df_holidays_events["type"]=="Work Day",1,holiday["work_day"])
        # Add local_holiday features
        # Link df_holidays_events["locale_name"] to the state or city where the store is located
        holiday["local_holiday"] = np.where(((df_holidays_events["type"] == "Holiday") 
                                             & ((df_holidays_events["locale_name"] == df_stores['state'][store]) 
                                                | (df_holidays_events["locale_name"] == df_stores['city'][store]))), 1, 0)
        holidays_per_store.append(holiday)
    return holidays_per_store

# Remove...
def remove_0_and_duplicates(holidays_per_store):
    listofseries = []
    for i in range(len(holidays_per_store)):
        df_holiday_per_store = holidays_per_store[i].set_index('date')
        df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store==0).all(axis=1)]
        df_holiday_per_store = df_holiday_per_store.groupby('date').agg({'national_holiday':'max', 'earthquake_relief':'max', 
                                   'christmas':'max', 'football_event':'max', 
                                    'work_day':'max', 
                                   'local_holiday':'max'}).reset_index()
        listofseries.append(df_holiday_per_store)
    return listofseries

# Assign holidays to 54 stores
def holiday_ts_54_store(holidays_per_store, sdate = '20130101', edate = '20170831'):
    listofseries = []
    for i in tqdm(range(0,54)):
        holidays_ts = TimeSeries.from_dataframe(holidays_per_store[i], 
                                        time_col = 'date',
                                        fill_missing_dates=True,
                                        fillna_value=0,
                                        freq='D')
        holidays_ts = holidays_ts.slice(pd.Timestamp(sdate),pd.Timestamp(edate)).astype(np.float32)
        listofseries.append(holidays_ts)
    return listofseries

# Assemble holiday_list(), remove_0_and_duplicates() and holiday_ts_54_store() to create holiday time series for 54 stores
# Transform the data
def create_transform_holiday_54_store(df_stores, df_holidays_events):
    temp_list1 = holiday_list(df_stores, df_holidays_events)
    temp_list2 = remove_0_and_duplicates(temp_list1)
    holidays_per_store = holiday_ts_54_store(temp_list2)
    holidays_transform = easy_data_transform(holidays_per_store)
    return holidays_transform

# Create and transform 7-day and 28-day moving average of promotion per family
def prom_transform_moving_avg(df_prom, family_list, col_names_new_7 = "prom_ma_7", col_names_new_28 = "prom_ma_28"):
    family_prom_dict = {}
    prom_transform_dict = {}
    for family in family_list:
        df_family = df_prom.loc[df_prom['family'] == family]
        ts_prom_list = TimeSeries.from_group_dataframe(
                                df_family,
                                time_col="date",
                                group_cols=["store_nbr","family"],
                                value_cols="onpromotion",
                                fill_missing_dates=True,
                                freq='D')
        for ts in ts_prom_list:
                ts = ts.astype(np.float32)
        family_prom_dict[family] = ts_prom_list
    for family in tqdm(family_prom_dict):
        prom_transform = easy_data_transform(family_prom_dict[family])
        # Moving Averages for Promotion Family Dictionaries
        promo_moving_avg_7 = MovingAverage(window=7)
        promo_moving_avg_28 = MovingAverage(window=28)
        prom_covar = []
        for ts in prom_transform:
            prom_avg_7 = promo_moving_avg_7.filter(ts)
            prom_avg_7 = prom_avg_7.from_series(prom_avg_7.pd_series()).astype(np.float32)
            prom_avg_7 = prom_avg_7.with_columns_renamed(col_names=prom_avg_7.components, col_names_new=col_names_new_7)
            prom_avg_28 = promo_moving_avg_28.filter(ts)
            prom_avg_28 = TimeSeries.from_series(prom_avg_28.pd_series()).astype(np.float32)
            prom_avg_28 = prom_avg_28.with_columns_renamed(col_names=prom_avg_28.components, col_names_new=col_names_new_28)
            prom_and_mas = ts.stack(prom_avg_7).stack(prom_avg_28)
            prom_covar.append(prom_and_mas)
        prom_transform_dict[family] = prom_covar
    return prom_transform_dict

# Combine store covar with general covar together
def stack_store_glocal_feature(store_list, general_covar, holidays_transform, transactions_covar):
    store_covar_future = []
#     store_covar_past = []
    holidays_transform_slice = holidays_transform
    for store in tqdm(range(len(store_list))):
        stack_future = holidays_transform[store].stack(general_covar)
        store_covar_future.append(stack_future)
#         holidays_transform_slice[store] = holidays_transform[store].slice_intersect(transactions_covar[store])
#         general_covar_slice = general_covar.slice_intersect(transactions_covar[store])
#         stack_past = transactions_covar[store].stack(holidays_transform_slice[store]).stack(general_covar_slice)  
#         store_covar_past.append(stack_past)
    return store_covar_future

# Assemble all future covariates and group in 33 family
def combine_all_covar(prom_transform_dict, store_covar_future):
    future_covar_dict = {}
    for family in tqdm(prom_transform_dict):
        prom_family = prom_transform_dict[family]
        covar_future = [prom_family[i].stack(store_covar_future[i]) for i in range(len(prom_family))]
        future_covar_dict[family] = covar_future
    return future_covar_dict

# Train 33 Global LightGBM Models (families) with Full Data
def LGBM_model_training(family_list, ts_fam_transform_dict, transactions_transform, future_covar_dict):
    LGBM_training_models = {}
    LGBM_training_errors={}
    for family in tqdm(family_list):
        # Define sales data for 33 families
        sales_family = ts_fam_transform_dict[family]
        training_data = [ts for ts in sales_family] 
        TCN_covar = future_covar_dict[family]
        training_slice = [training_data[i][:-16].slice_intersect(TCN_covar[i]) for i in range(len(training_data))]
        LGBM_training_model_per_family = LightGBMModel(lags = 65,
                             lags_future_covariates = (14,1),
                             lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22],
                             output_chunk_length=1,
                             random_state=2022,
                             gpu_use_dp= "false")
        LGBM_training_model_per_family.fit(series=training_slice, 
                        future_covariates=TCN_covar,
                        past_covariates=transactions_transform)
        LGBM_training_models[family] = LGBM_training_model_per_family
        
    return LGBM_training_models,LGBM_training_errors

# Use 33 Global LightGBM Models (families) to make prediction
def LGBM_model_predicting(family_list, ts_fam_transform_dict, transactions_transform, future_covar_dict, LGBM_training_models):
    LGBM_predicting_models = {}
    for family in tqdm(family_list):
        sales_family = ts_fam_transform_dict[family]
        training_data = [ts for ts in sales_family]
        LGBM_covar = future_covar_dict[family]
        TCN_covar = future_covar_dict[family]
        training_slice = [training_data[i][:-16].slice_intersect(TCN_covar[i]) for i in range(len(training_data))]
        LGBM_predicting_model_per_family = LGBM_training_models[family].predict(n=16,
                                                     series=training_slice,
                                                     future_covariates=LGBM_covar,
                                                     past_covariates=transactions_transform)
        LGBM_predicting_models[family] = LGBM_predicting_model_per_family
    return LGBM_predicting_models

# After prediction, transform Back
def LGBM_back_transform(family_list, pl_fam_dict, ts_fam_dict, LGBM_predicting_models):
    LGBM_back_transform_models = {}
    for family in tqdm(family_list):
        LGBM_back_transform_models[family] = pl_fam_dict[family].inverse_transform(LGBM_predicting_models[family], partial=True)
    for family in tqdm(LGBM_back_transform_models):
        for i in range(len(LGBM_back_transform_models[family])):
            if (ts_fam_dict[family][i].univariate_values()[-21:] == 0).all():
                LGBM_back_transform_models[family][i] = LGBM_back_transform_models[family][i].map(lambda x: x * 0)
    return LGBM_back_transform_models

def get_rmsle(stores_list, family_list, LGBM_back_transform_models,ts_fam_dict):
    error_per_family=[]
    for family in tqdm(family_list):
        pred_per_family=LGBM_back_transform_models[family]
        actual_per_family=ts_fam_dict[family]
        actual_sliced=[actual_per_family[i][-16:] for i in range(len(stores_list))]
        error_per_family.append(rmsle(actual_series=actual_sliced,
        pred_series=pred_per_family,reduction=np.mean))
    return np.mean(error_per_family)

def submission(store_list, family_list, LGBM_back_transform_models, df_test_sorted):
    listofseries = []
    for store in range(len(store_list)):
        for family in tqdm(family_list):
            pred = LGBM_back_transform_models[family][store].pd_dataframe()
            pred.columns = ['fcast']
            listofseries.append(pred)
    df_preds = pd.concat(listofseries)
    df_preds.reset_index(drop=True, inplace=True)

    # No Negative Forecasts
    df_preds[df_preds < 0] = 0
    sub = pd.concat([df_test_sorted, df_preds.set_index(df_test_sorted.index)], axis=1)
    sub_sorted = sub.sort_values(by=['id'])
    sub_sorted = sub_sorted.drop(['date','store_nbr','family'], axis=1)
    sub_sorted = sub_sorted.rename(columns={"fcast": "sales"})
    sub_sorted = sub_sorted.reset_index(drop=True)
    return sub_sorted

def main():
    # path = '../input/store-sales-time-series-forecasting/'
    path = 'data/'
    

    start_time=time.time()
    # Global time range
    sdate = '2013-01-01'
    edate = '2017-08-31'
    
    # Read a bunch of CSV files
    df_train, df_test, df_holidays_events, df_oil, df_stores, df_transactions = read_csv(path)
    
    # Find the unique family types
    family_list = df_train['family'].unique()
    
    # Find the unique store types
    store_list = df_stores['store_nbr'].unique()
    
    #Merge the training dataset with stores
    train_merged = pd.merge(df_train, df_stores, on ='store_nbr').sort_values(["store_nbr","family","date"]).astype({"store_nbr":'str', "family":'str', "city":'str',
                          "state":'str', "type":'str', "cluster":'str'})
    # Drop 'onpromotion' column in the testing dataset
    df_test_sorted = df_test.drop(['onpromotion'], axis=1).sort_values(by=['store_nbr','family'])
    
    # Create 33 TimeSeries objects that are grouped by product Family
    ts_fam_dict = {}
    print('Create time series grouped by 33 product family')
    ts_fam_dict = ts_fam(ts_fam_dict, family_list, train_merged)
    
    # Transform 33 groups of sales data
    # Meanwhile, document corresponding pipelines for 33 familes. These will be used at the end of lightGBM model
    pl_fam_dict = {}
    ts_fam_transform_dict = {}
    print('Transform sales data for 33 families')
    ts_fam_transform_dict, pl_fam_dict = fam_sales_transform(ts_fam_dict, pl_fam_dict, ts_fam_transform_dict)
        
    # Create 7-day and 28-day sales moving average
    print('Create moving avg sale data for 33 families')
    sales_moving_avg_dict = {}
    sales_moving_avg_dict = sales_moving_avg(sales_moving_avg_dict, ts_fam_transform_dict)
    
    # Create and transform time-based covariates
    print('Create global time based features')
    time_covar_transform = time_based_transform_covar(sdate, edate)
    
    # Create and transform oil price features
    # Also create 7-day and 28-day oil moving average
    print('Create global moving avg oil price')
    oil_transform, oil_moving_avg = oil_transform_moving_avg(df_oil)
    
    # Try to add global wages features???
    #holiday["wage"] = np.where(calendar['date']==15|end of the month,1,0)
        
    # Stack time-based covariates & oil price features together
    general_covar = time_covar_transform.stack(oil_transform).stack(oil_moving_avg)
    
    # Create individual moving avg transaction time series for 54 stores
    print('Create moving avg transaction time series for 54 stores')
    transactions_transform, transactions_covar = transactions_transform_moving_avg(df_transactions)
    
    # Rephrase holiday events information
    df_holidays_events = rephrase_holiday(df_holidays_events)
    
    # Create and transform holiday features for 54 stores
    print('Create and Transform holiday data for 54 stores')
    holidays_transform = create_transform_holiday_54_store(df_stores, df_holidays_events)
    
    # Create and transform 7-day and 28-day moving average of promotion per family
    print('Create moving avg promotion data for 33 families')
    df_prom = pd.concat([df_train, df_test], axis=0).sort_values(["store_nbr","family","date"])
    prom_transform_dict = prom_transform_moving_avg(df_prom, family_list)
    
    # Combine store covar with general covar together
    print('Combine store covar with general covar together for 54 stores')
    store_covar_future = stack_store_glocal_feature(store_list, general_covar, holidays_transform, transactions_covar)
    
    # Assemble All Covariates in Dictionaries and group in 33 family
    print('Assemble All Covariates in Dictionaries and group in 33 family')
    future_covar_dict = combine_all_covar(prom_transform_dict, store_covar_future)
    
    # Train 33 Global LightGBM Models (families) with Full Data
    print("Train 33 Global LightGBM Models (families)")
    LGBM_training_models = {}
    LGBM_training_models,training_errors = LGBM_model_training(family_list, ts_fam_transform_dict, 
                                               transactions_transform, future_covar_dict)

    # Use 33 Global LightGBM Models (families) to make prediction
    print("Use 33 Global LightGBM Models (families) to make prediction")
    LGBM_predicting_models = {}
    LGBM_predicting_models = LGBM_model_predicting(family_list, ts_fam_transform_dict, 
                                                   transactions_transform, future_covar_dict, LGBM_training_models)
    
    # After making prediction, transform back
    print("Transform back 33 Global LightGBM Models (families)")
    LGBM_back_transform_models = {}
    LGBM_back_transform_models = LGBM_back_transform(family_list, pl_fam_dict, 
                                                     ts_fam_dict, LGBM_predicting_models)
    print("validation score is")
    mean_error=get_rmsle(store_list,family_list,LGBM_back_transform_models,ts_fam_dict)
    print(mean_error)
    # Submission
    print("Generating 'submission.csv'")
    sub = submission(store_list, family_list, LGBM_back_transform_models, df_test_sorted)
    sub.to_csv('submission.csv', index=False)
    print('execution time',time.time()-start_time)
main()

